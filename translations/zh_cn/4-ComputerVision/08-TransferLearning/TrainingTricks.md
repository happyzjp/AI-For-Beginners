# 深度学习训练技巧



随着神经网络变得越来越深，其训练过程也变得越来越具有挑战性。一个主要问题是所谓的梯度消失或梯度爆炸。这篇文章很好地介绍了这些问题。

为了使深度网络的训练更有效，可以使用一些技术。

## 将值保持在合理区间内



为了使数值计算更稳定，我们希望确保神经网络中的所有值都在合理的范围内，通常为 [-1..1] 或 [0..1]。这不是一个非常严格的要求，但浮点计算的性质使得不同大小的值无法准确地一起处理。例如，如果我们添加 10 -10 和 10 10 ，我们很可能会得到 10 10 ，因为较小的值将“转换为”与较大值相同的阶，因此尾数将丢失。

大多数激活函数在 [-1..1] 附近具有非线性，因此将所有输入数据缩放至 [-1..1] 或 [0..1] 区间是有意义的。

## 初始权重初始化



理想情况下，我们希望在通过网络层后，值处于相同的范围内。因此，以保持值分布的方式初始化权重非常重要。

正态分布 N(0,1) 不是一个好主意，因为如果我们有 n 个输入，则输出的标准差将为 n，并且值很可能跳出 [0..1] 区间。

通常使用以下初始化：

- 均匀分布 -- `uniform`
- **N(0,1/n)** -- `gaussian`
- N(0,1/√n_in) 保证对于均值为零、标准差为 1 的输入，均值/标准差保持不变
- N(0,√2/(n_in+n_out)) -- 所谓的 Xavier 初始化 ( `glorot` )，它有助于在正向和反向传播期间保持信号范围

##  批量归一化



即使权重初始化正确，权重在训练过程中也会变得任意大或小，并且它们会将信号带出适当的范围。我们可以使用一种归一化技术来恢复信号。虽然有几种归一化技术（权重归一化、层归一化），但最常用的归一化技术是批归一化。

批归一化的思想是考虑小批量中的所有值，并基于这些值执行归一化（即减去均值并除以标准差）。它被实现为一个网络层，该网络层在应用权重后但在激活函数之前执行此归一化。因此，我们可能会看到更高的最终准确度和更快的训练。

以下是有关批归一化的原始论文、维基百科上的解释以及一篇很好的介绍性博客文章（以及俄语文章）。

##  辍学



Dropout 是一种有趣的技术，它在训练期间会移除一定比例的随机神经元。它还被实现为一个具有一个参数（要移除的神经元百分比，通常为 10%-50%）的层，并且在训练期间，它会在将输入向量传递到下一层之前将输入向量的随机元素归零。

虽然这听起来像一个奇怪的想法，但您可以在 `Dropout.ipynb` 笔记本中看到 dropout 对训练 MNIST 数字分类器产生的影响。它加快了训练速度，并使我们能够在更少的训练轮次中获得更高的准确度。

这种效果可以用几种方式来解释：

- 可以将其视为对模型的随机冲击因子，它将优化从局部最小值中剔除
- 可以将其视为隐式模型平均，因为我们可以说在 dropout 期间，我们正在训练略有不同的模型

> *有人说，当一个醉酒的人试图学习一些东西时，他第二天早上会记得更好，与一个清醒的人相比，因为一个大脑中有一些神经元功能失常，它会更好地适应理解含义。我们从未自己测试过这是否属实*

##  防止过拟合



深度学习的一个非常重要的方面是防止过拟合。虽然使用非常强大的神经网络模型可能很诱人，但我们应该始终平衡模型参数的数量和训练样本的数量。

> 确保您理解我们之前介绍的过拟合概念！

有几种方法可以防止过拟合：

- 早期停止——持续监控验证集上的错误，并在验证错误开始增加时停止训练。
- 显式权重衰减/正则化——对权重的绝对值较高而导致的损失函数增加额外的惩罚，这可以防止模型获得非常不稳定的结果
- 模型平均——训练多个模型，然后对结果求平均值。这有助于最小化方差。
- Dropout（隐式模型平均）

## 优化器/训练算法



训练的另一个重要方面是选择好的训练算法。虽然经典梯度下降是一个合理的选择，但它有时可能太慢，或导致其他问题。

在深度学习中，我们使用随机梯度下降 (SGD)，这是一种应用于从训练集中随机选择的迷你批次的梯度下降。使用此公式调整权重：

wt+1 = wt - η∇ℒ

###  动量



在动量 SGD 中，我们保留了来自先前步骤的一部分梯度。这类似于当我们带着惯性移动到某个地方时，我们受到不同方向的打击，我们的轨迹不会立即改变，但会保留原始运动的一部分。在这里，我们引入另一个向量 v 来表示速度：

- vt+1 = γ vt - η∇ℒ
- wt+1 = wt+vt+1

这里的参数 γ 表示我们考虑惯性的程度：γ=0 对应于经典 SGD；γ=1 是一个纯运动方程。

###  Adam、Adagrad 等



由于在每一层中我们用某个矩阵 W 乘以信号，根据 ||W||，梯度可以减小并接近 0，或者无限上升。这是爆炸/消失梯度问题的本质。

解决此问题的方法之一是在方程中仅使用梯度的方向，而忽略绝对值，即

w t+1 = w t - η(∇ℒ/||∇ℒ||), 其中 ||∇ℒ|| = √∑(∇ℒ) 2

这种算法称为 Adagrad。使用相同思想的其他算法：RMSProp、Adam

> Adam 被认为是许多应用程序中非常高效的算法，因此如果您不确定使用哪种算法，请使用 Adam。

###  梯度裁剪



梯度裁剪是对上述思想的扩展。当 ||∇ℒ|| ≤ θ 时，我们在权重优化中考虑原始梯度，当 ||∇ℒ|| > θ 时，我们将其梯度除以其范数。这里的 θ 是一个参数，在大多数情况下，我们可以取 θ=1 或 θ=10。

###  学习率衰减



训练成功通常取决于学习率参数 η。可以合理地假设，较大的 η 值会导致更快的训练，这通常是我们希望在训练开始时实现的，然后较小的 η 值使我们能够对网络进行微调。因此，在大多数情况下，我们希望在训练过程中减小 η。

这可以通过在每次训练时代乘以 η 某个数字（例如 0.98）或使用更复杂的学习率计划来完成。

## 不同的网络架构



为您的问题选择正确的网络架构可能很棘手。通常，我们会采用一种已被证明适用于我们特定任务（或类似任务）的架构。以下是计算机视觉神经网络架构的良好概述。

> 选择一个功能足够强大的架构非常重要，以满足我们拥有的训练样本数量。选择功能太强大的模型会导致过度拟合

另一种好方法是使用一种架构，该架构将自动调整到所需的复杂性。在某种程度上，ResNet 架构和 Inception 是自调整的。更多关于计算机视觉架构的信息
