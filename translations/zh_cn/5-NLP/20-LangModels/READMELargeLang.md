# 预训练大型语言模型



在我们之前的所有任务中，我们都在使用标记数据集训练神经网络来执行特定任务。对于大型 Transformer 模型（例如 BERT），我们以自监督的方式使用语言建模来构建语言模型，然后通过进一步的特定领域训练将其专门用于特定下游任务。然而，事实证明，大型语言模型也可以在没有任何特定领域训练的情况下解决许多任务。能够做到这一点的一系列模型称为 GPT：生成式预训练 Transformer。

## [ 课前测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/120)



## 文本生成和困惑度



神经网络能够在没有下游训练的情况下执行一般任务的想法在语言模型是无监督多任务学习者论文中提出。其主要思想是，许多其他任务都可以使用文本生成来建模，因为理解文本本质上意味着能够生成文本。由于该模型是在包含人类知识的大量文本上进行训练的，因此它也变得对各种主题都有所了解。

> 理解并能够生成文本还包括了解我们周围的世界。人们在很大程度上也通过阅读来学习，GPT 网络在这方面是相似的。

文本生成网络通过预测下一个单词�(��)的概率来工作。然而，下一个单词的无条件概率等于该单词在文本语料库中的频率。GPT 能够根据前一个单词为我们提供下一个单词的条件概率：�(��|��−1,...,�0)

> 您可以在我们的数据科学初学者课程中阅读更多有关概率的信息

语言生成模型的质量可以使用困惑度来定义。这是一个内在指标，它允许我们在没有任何特定任务数据集的情况下衡量模型质量。它基于句子的概率概念——该模型为可能是真实的句子分配高概率（即模型不会对此感到困惑），并为不太有意义的句子分配低概率（例如，它能做什么？）。当我们从真实文本语料库中为我们的模型提供句子时，我们希望它们具有高概率和低困惑度。在数学上，它被定义为测试集的归一化逆概率：$ \mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)} $

您可以使用 Hugging Face 提供的基于 GPT 的文本编辑器来尝试文本生成。在此编辑器中，您开始编写文本，然后按 [TAB] 将为您提供几个完成选项。如果它们太短，或者您对它们不满意 - 再次按 [TAB]，您将有更多选项，包括更长的文本片段。

##  GPT 是一个家族



GPT 不是一个单一模型，而是一系列由 OpenAI 开发和训练的模型。

在 GPT 模型下，我们有：

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4)           |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------- |
| 拥有多达 15 亿个参数的语言模型。                             | 拥有多达 1750 亿个参数的语言模型                             | 100T 个参数，接受图像和文本输入并输出文本。 |

GPT-3 和 GPT-4 模型可作为 Microsoft Azure 的认知服务和 OpenAI API 获得。

##  提示工程



由于 GPT 已接受过大量数据的训练以理解语言和代码，因此它们会针对输入（提示）提供输出。提示是 GPT 输入或查询，人们借此向模型提供有关接下来要完成的任务的说明。要引出所需的结果，您需要最有效的提示，其中涉及选择正确的单词、格式、短语甚至符号。这种方法就是提示工程

本说明书将为您提供有关提示工程的更多信息。

## ✍️ 示例笔记本：使用 OpenAI-GPT 玩耍



在以下笔记本中继续学习：

- [使用 OpenAI-GPT 和 Hugging Face Transformers 生成文本](https://github.com/happyzjp/AI-For-Beginners/blob/main/translations/zh_cn/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

##  结论



新的通用预训练语言模型不仅可以模拟语言结构，还包含大量的自然语言。因此，它们可以有效地用于解决零商店或少商店设置中的一些 NLP 任务。

## [ 课后测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/220)
